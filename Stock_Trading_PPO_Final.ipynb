{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library import & Drive mount"
      ],
      "metadata": {
        "id": "3lGJ1SdFUjyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-asTv-eoTkVT",
        "outputId": "be85b710-dd62-4b5b-8e61-e456e6d300c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Reinforcement_Learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIe3lzcZTqRA",
        "outputId": "06093b55-b3ce-4e9b-c4b6-3f1ac202d129"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Reinforcement_Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8IRpZhCX8LTF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import torch.optim as optim\n",
        "import importlib\n",
        "from calculate_tech_ind import calculate_macd, calculate_rsi, calculate_cci, calculate_adx\n",
        "from model import PreLSTM, PolicyNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl_zdekM2AjR",
        "outputId": "5e3724cc-fdbe-4d65-ff22-4ab2dc15e180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data download and Preprocessing"
      ],
      "metadata": {
        "id": "f3Upl-IAT4Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbols = {\n",
        "    'S&P 500': '^GSPC',\n",
        "    'Dow Jones': '^DJI',\n",
        "    'KOSPI': '^KS11'\n",
        "}\n",
        "start_date = '2012-12-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "market_data = {}\n",
        "for name, ticker in symbols.items():\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    market_data[name] = data\n",
        "\n",
        "snp500_data = market_data['S&P 500']\n",
        "dowjones_data = market_data['Dow Jones']\n",
        "kospi_data = market_data['KOSPI']\n",
        "#print(snp500_data)\n",
        "\n",
        "# 공통 거래일 계산\n",
        "common_dates = snp500_data.index.intersection(dowjones_data.index).intersection(kospi_data.index)\n",
        "\n",
        "# 공통 거래일 기준 데이터 정렬\n",
        "snp500_data_aligned = snp500_data.loc[common_dates]\n",
        "dowjones_data_aligned = dowjones_data.loc[common_dates]\n",
        "kospi_data_aligned = kospi_data.loc[common_dates]\n",
        "\n",
        "snp500_data_aligned = calculate_macd(snp500_data_aligned)\n",
        "snp500_data_aligned = calculate_rsi(snp500_data_aligned)\n",
        "snp500_data_aligned = calculate_cci(snp500_data_aligned)\n",
        "\n",
        "dowjones_data_aligned = calculate_macd(dowjones_data_aligned)\n",
        "dowjones_data_aligned = calculate_rsi(dowjones_data_aligned)\n",
        "dowjones_data_aligned = calculate_cci(dowjones_data_aligned)\n",
        "\n",
        "kospi_data_aligned = calculate_macd(kospi_data_aligned)\n",
        "kospi_data_aligned = calculate_rsi(kospi_data_aligned)\n",
        "kospi_data_aligned = calculate_cci(kospi_data_aligned)\n",
        "\n",
        "# Close, MACD, RSI 병합\n",
        "merged_close = pd.concat([\n",
        "    snp500_data_aligned[['Close']].rename(columns={'Close': 'S&P 500 Close'}),\n",
        "    dowjones_data_aligned[['Close']].rename(columns={'Close': 'Dow Jones Close'}),\n",
        "    kospi_data_aligned[['Close']].rename(columns={'Close': 'KOSPI Close'})\n",
        "], axis=1)\n",
        "\n",
        "merged_macd = pd.concat([\n",
        "    snp500_data_aligned[['MACD']].rename(columns={'MACD': 'S&P 500 MACD'}),\n",
        "    dowjones_data_aligned[['MACD']].rename(columns={'MACD': 'Dow Jones MACD'}),\n",
        "    kospi_data_aligned[['MACD']].rename(columns={'MACD': 'KOSPI MACD'})\n",
        "], axis=1)\n",
        "\n",
        "merged_rsi = pd.concat([\n",
        "    snp500_data_aligned[['RSI']].rename(columns={'RSI': 'S&P 500 RSI'}),\n",
        "    dowjones_data_aligned[['RSI']].rename(columns={'RSI': 'Dow Jones RSI'}),\n",
        "    kospi_data_aligned[['RSI']].rename(columns={'RSI': 'KOSPI RSI'})\n",
        "], axis=1)\n",
        "\n",
        "merged_cci = pd.concat([\n",
        "    snp500_data_aligned[['CCI']].rename(columns={'CCI': 'S&P 500 CCI'}),\n",
        "    dowjones_data_aligned[['CCI']].rename(columns={'CCI': 'Dow Jones CCI'}),\n",
        "    kospi_data_aligned[['CCI']].rename(columns={'CCI': 'KOSPI CCI'})\n",
        "], axis=1)\n",
        "\n",
        "# 전체 병합 데이터 생성\n",
        "merged_data_all = pd.concat([merged_close, merged_macd, merged_rsi, merged_cci], axis=1)\n",
        "\n",
        "# 날짜 기준 필터링\n",
        "merged_data_all = merged_data_all[merged_data_all.index > '2013-02-01']\n",
        "\n",
        "print(merged_data_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gdLNLmDfg6dl",
        "outputId": "9080e492-3782-434e-9510-62cbc352d9d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price      S&P 500 Close Dow Jones Close  KOSPI Close S&P 500 MACD  \\\n",
            "Ticker             ^GSPC            ^DJI        ^KS11                \n",
            "Date                                                                 \n",
            "2013-02-04   1495.709961    13880.080078  1953.209961    16.833122   \n",
            "2013-02-05   1511.290039    13979.299805  1938.180054    16.887413   \n",
            "2013-02-06   1512.119995    13986.519531  1936.189941    16.803706   \n",
            "2013-02-07   1509.390015    13944.049805  1931.770020    16.328852   \n",
            "2013-02-08   1517.930054    13992.969727  1950.900024    16.451989   \n",
            "...                  ...             ...          ...          ...   \n",
            "2023-12-21   4746.750000    37404.351562  2600.020020    76.724339   \n",
            "2023-12-22   4754.629883    37385.968750  2599.510010    76.666913   \n",
            "2023-12-26   4774.750000    37545.328125  2602.590088    77.353248   \n",
            "2023-12-27   4781.580078    37656.519531  2613.500000    77.554305   \n",
            "2023-12-28   4783.350098    37710.101562  2655.280029    76.969218   \n",
            "\n",
            "Price      Dow Jones MACD KOSPI MACD S&P 500 RSI Dow Jones RSI  KOSPI RSI  \\\n",
            "Ticker                                                                      \n",
            "Date                                                                        \n",
            "2013-02-04     180.360074  -7.470035   64.728699     70.818071  31.785890   \n",
            "2013-02-05     180.065726  -9.053764   69.693633     72.980479  33.670288   \n",
            "2013-02-06     178.359013 -10.350155   69.858200     75.002312  34.741143   \n",
            "2013-02-07     171.601365 -11.600483   65.157209     69.161056  34.425410   \n",
            "2013-02-08     168.253785 -10.921844   66.409903     68.998543  36.988559   \n",
            "...                   ...        ...         ...           ...        ...   \n",
            "2023-12-21     700.546982  33.386099   69.835202     73.243639  71.519845   \n",
            "2023-12-22     687.566322  34.442140   75.219000     73.916007  70.007602   \n",
            "2023-12-26     682.273204  35.122724   77.026599     77.862195  77.953871   \n",
            "2023-12-27     679.220951  36.125993   81.131399     80.920028  79.016433   \n",
            "2023-12-28     673.363525  39.833217   79.209536     80.850790  83.719638   \n",
            "\n",
            "Price      S&P 500 CCI Dow Jones CCI   KOSPI CCI  \n",
            "Ticker                                            \n",
            "Date                                              \n",
            "2013-02-04   73.746353     84.625677  -71.733645  \n",
            "2013-02-05   91.400688     88.549134 -131.124200  \n",
            "2013-02-06   95.746782     83.338664 -111.608750  \n",
            "2013-02-07   76.991980     65.449148 -107.568728  \n",
            "2013-02-08  112.431485     82.164587  -71.018357  \n",
            "...                ...           ...         ...  \n",
            "2023-12-21  103.481750    100.841129  174.089461  \n",
            "2023-12-22  106.663068     97.038802  158.674305  \n",
            "2023-12-26  107.646047     97.460350  128.034459  \n",
            "2023-12-27  100.772878     97.615953  114.320801  \n",
            "2023-12-28   97.092625    100.516152  151.994028  \n",
            "\n",
            "[2597 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "F-vGveYJUFuF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QpJRQCMswFJu"
      },
      "outputs": [],
      "source": [
        "class env():\n",
        "  def __init__(self, data:pd.DataFrame, train_split_date = '2021-01-01', valid_split_date = '2022-01-01', test_split_date  = '2023-01-01'):\n",
        "    self.data = data\n",
        "    self.t = 0\n",
        "    self.train_data = torch.tensor(self.data[self.data.index < train_split_date].values).to(torch.float32).to(device)\n",
        "    self.valid_data = torch.tensor(self.data[(self.data.index >= train_split_date) & (self.data.index < valid_split_date)].values).to(torch.float32).to(device)\n",
        "    self.test_data = torch.tensor(self.data[self.data.index >= test_split_date].values).to(torch.float32).to(device)\n",
        "\n",
        "  def step(self, state, action, data): #데이터 앞의 세개는 각 인덱스의 가격으로 설정\n",
        "      shares_to_buy = torch.round(action).to(torch.float32).to(device)\n",
        "      num_stock, balance = state[-4:-1], state[-1]\n",
        "      W = torch.dot(state[:3], num_stock) + balance\n",
        "      #print(shares_to_buy.device, num_stock.device, balance.device)\n",
        "      shares_to_buy = torch.where(shares_to_buy + num_stock < 0, -num_stock, shares_to_buy)\n",
        "      cost = torch.dot(data[self.t,:3], shares_to_buy)\n",
        "\n",
        "      #print(f\"shares_to_buy: {shares_to_buy}, balance: {balance}, cost: {cost}\")\n",
        "      if cost > balance:\n",
        "          cost = torch.tensor(0).to(device)\n",
        "          shares_to_buy = torch.zeros(3).to(device)\n",
        "      n_balance = balance - cost\n",
        "      self.t += 1\n",
        "\n",
        "      done = self.t >= (data.shape[0] - 1)\n",
        "\n",
        "      n_num_stock = num_stock + shares_to_buy\n",
        "      n_W = torch.dot(data[self.t,:3], n_num_stock) + n_balance\n",
        "\n",
        "      n_state = torch.hstack((data[self.t, :], n_num_stock, n_balance))\n",
        "      reward = n_W - W\n",
        "\n",
        "      #print(f\"n_balance: {n_balance}, n_stock: {n_num_stock}, n_W: {n_W}, reward: {reward}\")\n",
        "      return n_state, reward, done\n",
        "\n",
        "  def train_step(self, state, action):\n",
        "      return self.step(state, action, self.train_data)\n",
        "\n",
        "  def test_step(self, state, action):\n",
        "      return self.step(state, action, self.test_data)\n",
        "\n",
        "  def valid_step(self, state, action):\n",
        "      return self.step(state, action, self.valid_data)\n",
        "\n",
        "  def reset(self, data):\n",
        "      self.t = 0\n",
        "      init_S = torch.zeros(3).to(device)\n",
        "      init_B = torch.tensor(1e8).to(device)\n",
        "      init_state = torch.hstack((data[0,:], init_S, init_B))\n",
        "      return init_state\n",
        "\n",
        "  def train_reset(self):\n",
        "      return self.reset(self.train_data)\n",
        "\n",
        "  def test_reset(self):\n",
        "      return self.reset(self.test_data)\n",
        "\n",
        "  def valid_reset(self):\n",
        "      return self.reset(self.valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO Agent"
      ],
      "metadata": {
        "id": "ggI5SIURUKdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import model\n",
        "import importlib\n",
        "importlib.reload(model)\n",
        "\n",
        "GAMMA = 0.99\n",
        "LAM = 0.95  # Lambda for GAE\n",
        "LEARNING_RATE = 0.0001\n",
        "CLIP_EPSILON = 0.1\n",
        "PPO_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "K = 100  # 최대 매수/매도 개수\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, window_size, num_stocks, feature_len, n_channels, k=K):\n",
        "        self.num_stocks = num_stocks\n",
        "        self.feature_extractor = model.PreLSTM(n_channels, hidden_dim=128, output_dim=feature_len).to(device)\n",
        "        self.policy_network = model.PolicyNetwork(self.feature_extractor, num_stocks).to(device)\n",
        "        self.value_network = model.ValueNetwork(self.feature_extractor).to(device)\n",
        "\n",
        "        self.policy_optimizer = optim.Adam(list(self.feature_extractor.parameters()) + list(self.policy_network.parameters()), lr=LEARNING_RATE)\n",
        "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        self.gamma = GAMMA\n",
        "        self.lam = LAM\n",
        "        self.clip_epsilon = CLIP_EPSILON\n",
        "        self.k = k\n",
        "\n",
        "        # Trajectory buffer\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "\n",
        "        self.state_buffer = deque(maxlen=window_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        self.state_buffer.append(state)\n",
        "        if len(self.state_buffer) < self.state_buffer.maxlen:\n",
        "            return torch.zeros(self.num_stocks).to(device), 0., 0.\n",
        "\n",
        "        state_tensor = torch.stack(list(self.state_buffer)).to(torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma_diag = self.policy_network(state_tensor)\n",
        "            sigma = torch.diag(torch.clamp(sigma_diag, min=1e-6))\n",
        "            dist = torch.distributions.MultivariateNormal(mu, covariance_matrix=sigma)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            clipped_action = torch.clamp(action, -1, 1)\n",
        "            discrete_action = (clipped_action * self.k).long()\n",
        "\n",
        "            value = self.value_network(state_tensor)\n",
        "        #print(f\"actions: {discrete_action.shape}, log_prob: {log_prob}, value: {value.shape}\")\n",
        "        return discrete_action.cpu(), log_prob.cpu().item(), value.cpu().item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, done, log_prob, value):\n",
        "        # Calculate after the trajectory\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value=0.0):\n",
        "        values = self.values + [last_value]\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for step in reversed(range(len(self.rewards))):\n",
        "            delta = self.rewards[step] + self.gamma * values[step+1]*(1 - self.dones[step]) - values[step]\n",
        "            gae = delta + self.gamma * self.lam * (1 - self.dones[step]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "        returns = [adv + val for adv, val in zip(advantages, self.values)]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        # Normalize Advantage\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "        return returns, advantages\n",
        "\n",
        "    def ppo_update(self, states, actions, log_probs, returns, advantages):\n",
        "        # PPO update\n",
        "        dataset_size = states.shape[0]\n",
        "        for _ in range(PPO_EPOCHS):\n",
        "            indices = np.arange(dataset_size)\n",
        "            np.random.shuffle(indices)\n",
        "            for start in range(0, dataset_size, BATCH_SIZE):\n",
        "                end = start + BATCH_SIZE\n",
        "                batch_idx = indices[start:end]\n",
        "\n",
        "                batch_states = states[batch_idx].to(device)\n",
        "                batch_actions = actions[batch_idx].to(torch.float32).to(device)\n",
        "                batch_log_probs_old = log_probs[batch_idx].to(device)\n",
        "                batch_returns = returns[batch_idx].to(device)\n",
        "                batch_advantages = advantages[batch_idx].to(device)\n",
        "\n",
        "                mu, sigma_diag = self.policy_network(batch_states)\n",
        "                sigma = []\n",
        "                for s_diag in sigma_diag:\n",
        "                    sigma.append(torch.diag(torch.clamp(s_diag, min=1e-6)))\n",
        "                sigma = torch.stack(sigma) # (batch_size, num_stocks, num_stocks)\n",
        "                dist = torch.distributions.MultivariateNormal(mu, covariance_matrix=sigma)\n",
        "                new_log_probs = dist.log_prob(batch_actions)\n",
        "\n",
        "                ratio = torch.exp(new_log_probs - batch_log_probs_old)\n",
        "\n",
        "                # Clipped objective\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * batch_advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value = self.value_network(batch_states).squeeze()\n",
        "                value_loss = nn.MSELoss()(value, batch_returns)\n",
        "\n",
        "                total_loss = policy_loss + 0.5 * value_loss\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                self.value_optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.policy_optimizer.step()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "    def finish_trajectory_and_update(self, last_state=None, done=False):\n",
        "        # Update\n",
        "        if len(self.states) == 0:\n",
        "            return\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if last_state is not None:\n",
        "                state_tensor = torch.stack(list(self.state_buffer)).to(torch.float32).to(device)\n",
        "                last_value = self.value_network(state_tensor).item() if not done else 0.0\n",
        "            else:\n",
        "                last_value = 0.0\n",
        "\n",
        "        returns, advantages = self.compute_returns_and_advantages(last_value)\n",
        "\n",
        "        states_tensor = torch.stack(self.states).to(torch.float32)\n",
        "        actions_tensor = torch.stack(self.actions).to(torch.float32)\n",
        "        log_probs_tensor = torch.tensor(self.log_probs, dtype=torch.float32)\n",
        "\n",
        "        self.ppo_update(states_tensor, actions_tensor, log_probs_tensor, returns, advantages)\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n"
      ],
      "metadata": {
        "id": "EDFuNFspwUJV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO Train"
      ],
      "metadata": {
        "id": "KLf36qREUbPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "LOqM9fRSBKWe",
        "outputId": "ff597b95-f409-458e-8322-da50459a853a",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  return disable_fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 1236992.0\n",
            "validation_reward: 274520.0\n",
            "Episode: 2, Total Reward: 9685024.0\n",
            "Episode: 3, Total Reward: 70895184.0\n",
            "Episode: 4, Total Reward: 86440000.0\n",
            "Episode: 5, Total Reward: 87056784.0\n",
            "Episode: 6, Total Reward: 87411728.0\n",
            "validation_reward: 9089744.0\n",
            "Episode: 7, Total Reward: 87735456.0\n",
            "Episode: 8, Total Reward: 90496256.0\n",
            "Episode: 9, Total Reward: 89104928.0\n",
            "Episode: 10, Total Reward: 89630320.0\n",
            "Episode: 11, Total Reward: 89100064.0\n",
            "validation_reward: 10849184.0\n",
            "Episode: 12, Total Reward: 90457984.0\n",
            "Episode: 13, Total Reward: 88922288.0\n",
            "Episode: 14, Total Reward: 87412192.0\n",
            "Episode: 15, Total Reward: 86548080.0\n",
            "Episode: 16, Total Reward: 86741904.0\n",
            "validation_reward: 10976280.0\n",
            "Episode: 17, Total Reward: 88667040.0\n",
            "Episode: 18, Total Reward: 87658480.0\n",
            "Episode: 19, Total Reward: 86345472.0\n",
            "Episode: 20, Total Reward: 85374688.0\n",
            "Episode: 21, Total Reward: 85681392.0\n",
            "validation_reward: 10672272.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-46cfda077f51>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5ab60a29dfe3>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mdiscrete_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclipped_action\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;31m#print(f\"actions: {discrete_action.shape}, log_prob: {log_prob}, value: {value.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdiscrete_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Reinforcement_Learning/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Reinforcement_Learning/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Pass through the linear layer to get the output features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPISODE = 500\n",
        "Env = env(merged_data_all)\n",
        "Agent =  PPOAgent(window_size = 30, num_stocks = 3, feature_len = 128, n_channels=16, k=20)\n",
        "\n",
        "for episode in range(EPISODE):\n",
        "    state = Env.train_reset().to(device)\n",
        "    total_reward = torch.tensor(0, device = device, dtype = torch.float32)\n",
        "    for i in range(Env.train_data.shape[0]-2):\n",
        "        action, log_prob, value = Agent.act(state.to(device))\n",
        "        next_state, reward, done = Env.train_step(state, action)\n",
        "\n",
        "        if len(Agent.state_buffer) == Agent.state_buffer.maxlen:\n",
        "            state_to_be_stored = torch.stack(list(Agent.state_buffer)).to(torch.float32).to(device)\n",
        "            Agent.store_transition(state_to_be_stored, action, reward, done, log_prob, value)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    Agent.finish_trajectory_and_update(last_state = state, done = done)\n",
        "\n",
        "    if episode % 1 == 0:\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    if episode % 5 == 0:\n",
        "        state = Env.valid_reset()\n",
        "        valid_reward = torch.tensor(0, device = device, dtype = torch.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(Env.test_data.shape[0]-2):\n",
        "                action, _, _ = Agent.act(state.to(device))\n",
        "                next_state, reward, done = Env.test_step(state, action)\n",
        "                state = next_state\n",
        "                valid_reward += reward\n",
        "\n",
        "        print(\"validation_reward:\",valid_reward.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO Test"
      ],
      "metadata": {
        "id": "XnbWF32RUeGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트는 트레이닝시와 동일하게 1억원의 잔고를 가지고 진행\n",
        "최종적으로 10.7퍼센트 가량의 수익을 올렸습니다"
      ],
      "metadata": {
        "id": "BRtcf1oGo7aN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gEXmVLRvlov",
        "outputId": "f060338d-7301-432c-a65e-6feaf9ca5854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward: 10761976.0\n"
          ]
        }
      ],
      "source": [
        "Env = env(merged_data_all)\n",
        "state = Env.test_reset()\n",
        "total_reward = torch.tensor(0, device = device, dtype = torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(Env.test_data.shape[0]-2):\n",
        "        action, _, _ = Agent.act(state.to(device))\n",
        "        next_state, reward, done = Env.test_step(state, action)\n",
        "        #print(f\"{i}th reward:{reward}\")\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "print(\"total_reward:\",total_reward.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zovwAyYqvdRh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "3lGJ1SdFUjyZ",
        "f3Upl-IAT4Yz",
        "F-vGveYJUFuF",
        "ggI5SIURUKdR",
        "KLf36qREUbPk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}